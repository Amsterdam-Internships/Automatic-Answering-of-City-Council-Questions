{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages(number_pages):\n",
    "    \"\"\"\n",
    "    Input: Number of pages to be scraped\n",
    "    Output: URLs of pages\n",
    "    \"\"\"\n",
    "    pages_l = []\n",
    "    for page in range(1, number_pages + 1):\n",
    "        page_url = 'https://www.rijksoverheid.nl/actueel/nieuws?pagina=' + str(page)\n",
    "        pages_l.append(page_url)\n",
    "    return pages_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_urls(page_url):\n",
    "    \"\"\"\n",
    "    Input: URL of page in https://www.rijksoverheid.nl/actueel/nieuws\n",
    "    Output: A list of the news document URLs on this page of len 10\n",
    "    \"\"\"\n",
    "    response = requests.get(page_url)\n",
    "    document_urls = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        links = soup.find_all(\"a\")\n",
    "\n",
    "        for link in links:\n",
    "                href = link.get(\"href\")\n",
    "                if href.startswith('/actueel/nieuws'):\n",
    "                    document_urls.append('https://www.rijksoverheid.nl'+ href)\n",
    "                    \n",
    "    return document_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example \n",
    "# get_document_urls('https://www.rijksoverheid.nl/actueel/nieuws?pagina=5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_content(url):\n",
    "    \"\"\"\n",
    "    Input: URL \n",
    "    Output: HTML content of URL\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        html_content = response.content.decode('utf-8')\n",
    "        return html_content\n",
    "    else:\n",
    "        return (f'Request failed with status code {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_html(html_content):\n",
    "    \"\"\"\n",
    "    Selects only the HTML content from 'class': 'article content'\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(test, 'html.parser')\n",
    "    div_element = soup.find('div', {'class': 'article content'})\n",
    "\n",
    "    html_content = div_element.decode_contents()\n",
    "\n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_documenten(article_html):\n",
    "    \"\"\"\n",
    "    Removes everything after '<div class=\"block docs-pubs results\">'\n",
    "    \"\"\"\n",
    "    substring = '<div class=\"block docs-pubs results\">'\n",
    "    new_content = article_html.split(substring, 1)[0]\n",
    "    \n",
    "    return new_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paragraphs(html_content):\n",
    "    \"\"\"\n",
    "    Input: HTML content \n",
    "    Output: A List of type(list) of HTML textual content splitted in paragraphs\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # find all <p> tags and extract their text contents\n",
    "    paragraphs = [p.get_text() for p in soup.find_all('p')]\n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# example = remove_documenten(get_article_html(test))\n",
    "# example\n",
    "# split_paragraphs(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_news_content(num_pages=5, clean_documenten = True, split_par = True):\n",
    "    \"\"\"\n",
    "    Input: num_pages = 5 (by default): set the num. of pages to scrape\n",
    "           clean_documenten = True (by default): set if <div class=\"block docs-pubs results\"> should be removed\n",
    "           split_par = True (by default): set if HTML content should be cleaned and slip into paragraphs\n",
    "           \n",
    "    Return: Dictionary type(dict) where key is article URL and value is its content \n",
    "    \"\"\"\n",
    "    url_content = {}\n",
    "    \n",
    "    pages = get_pages(num_pages)\n",
    "    \n",
    "    for page in pages: # iterate over pages\n",
    "        document_urls = get_document_urls(page)\n",
    "        \n",
    "        for doc_url in document_urls: # iterate over docs\n",
    "            content = get_html_content(doc_url) # collect HTML content\n",
    "            if clean_documenten:\n",
    "                content = remove_documenten(content)\n",
    "            if split_par:\n",
    "                content = split_paragraphs(content)\n",
    "            url_content[doc_url] = content\n",
    "    return url_content\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# collect_news_content(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
