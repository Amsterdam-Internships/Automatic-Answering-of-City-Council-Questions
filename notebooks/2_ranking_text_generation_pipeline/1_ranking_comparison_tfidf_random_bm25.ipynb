{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/natalipeeva/Documents/GitHub/Automatic-Answering-of-City-Council-Questions\n"
     ]
    }
   ],
   "source": [
    "%cd  /Users/natalipeeva/Documents/GitHub/Automatic-Answering-of-City-Council-Questions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "src_dir = os.path.join(os.getcwd(), 'src_clean')\n",
    "sys.path.append(src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from hashids import Hashids\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/natalipeeva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from src_clean.preprocessing.passages import create_passages_dataframe, add_passages_ids\n",
    "from src_clean.preprocessing.text_preprocessing import preprocess_text\n",
    "from src_clean.ranking.sparse_ranking import perform_tfidf_search, perform_random_search, perform_bm25_search\n",
    "from src_clean.ranking.evaluation import calculate_average_metrics_retrieval, calculate_metrics_answer_similarity, simulate_answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_for_ranking = pd.read_csv('data/amsterdam/amsterdam_questions.csv')\n",
    "collection = pd.read_csv('data/amsterdam/amsterdam_full.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for ranking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_df = create_passages_dataframe(collection) # 50 secs for execution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean passages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicates in general; drop duplictates in column 'Textual_Content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_df = passages_df.drop_duplicates() # drop dup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_df = passages_df.drop_duplicates(subset='Textual_Content', keep=False) # drop dup. passages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process Passages Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages_df['Preprocessed_Text'] = passages_df['Textual_Content'].apply(lambda x: preprocess_text(x,stem=True,\n",
    "                                                                                      remove_stopwords=True,\n",
    "                                                                                      lowercase_text=True,\n",
    "                                                                                      remove_punct=True)) # 40 seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Passage ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashids = Hashids()\n",
    "passages_df[\"id\"] = [hashids.encode(i) for i in range(len(passages_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Textual_Content</th>\n",
       "      <th>Preprocessed_Text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60893</th>\n",
       "      <td>https://www.amsterdam.nl/nieuws/kennisgevingen...</td>\n",
       "      <td>Postcode Toelichting Vul een geldige postcode ...</td>\n",
       "      <td>postcod toelicht vul geldig postcod volgend fo...</td>\n",
       "      <td>Wz8x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60894</th>\n",
       "      <td>https://www.amsterdam.nl/nieuws/kennisgevingen...</td>\n",
       "      <td>Wittenburgergracht 197 06 juni 2023 Besluit ap...</td>\n",
       "      <td>wittenburgergracht juni besluit apv vergunn ve...</td>\n",
       "      <td>XAMg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60895</th>\n",
       "      <td>https://www.amsterdam.nl/nieuws/kennisgevingen...</td>\n",
       "      <td>19 1071BJ , Vrijheidslaan 25 1079KB , Vrijheid...</td>\n",
       "      <td>1071bj vrijheidslan 1079kb vrijheidslan 1079ke...</td>\n",
       "      <td>YBM2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60896</th>\n",
       "      <td>https://www.amsterdam.nl/nieuws/kennisgevingen...</td>\n",
       "      <td>Hartplein 2A in AMSTERDAM 06 juni 2023 Besluit...</td>\n",
       "      <td>hartplein 2a amsterdam juni besluit apv vergun...</td>\n",
       "      <td>ZDW5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60897</th>\n",
       "      <td>https://www.amsterdam.nl/nieuws/kennisgevingen...</td>\n",
       "      <td>binnentuin en privétuinen van het bouwblok tus...</td>\n",
       "      <td>binnentuin privetuin bouwblok tuss tugelaweg m...</td>\n",
       "      <td>1z9m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     URL  \\\n",
       "60893  https://www.amsterdam.nl/nieuws/kennisgevingen...   \n",
       "60894  https://www.amsterdam.nl/nieuws/kennisgevingen...   \n",
       "60895  https://www.amsterdam.nl/nieuws/kennisgevingen...   \n",
       "60896  https://www.amsterdam.nl/nieuws/kennisgevingen...   \n",
       "60897  https://www.amsterdam.nl/nieuws/kennisgevingen...   \n",
       "\n",
       "                                         Textual_Content  \\\n",
       "60893  Postcode Toelichting Vul een geldige postcode ...   \n",
       "60894  Wittenburgergracht 197 06 juni 2023 Besluit ap...   \n",
       "60895  19 1071BJ , Vrijheidslaan 25 1079KB , Vrijheid...   \n",
       "60896  Hartplein 2A in AMSTERDAM 06 juni 2023 Besluit...   \n",
       "60897  binnentuin en privétuinen van het bouwblok tus...   \n",
       "\n",
       "                                       Preprocessed_Text    id  \n",
       "60893  postcod toelicht vul geldig postcod volgend fo...  Wz8x  \n",
       "60894  wittenburgergracht juni besluit apv vergunn ve...  XAMg  \n",
       "60895  1071bj vrijheidslan 1079kb vrijheidslan 1079ke...  YBM2  \n",
       "60896  hartplein 2a amsterdam juni besluit apv vergun...  ZDW5  \n",
       "60897  binnentuin privetuin bouwblok tuss tugelaweg m...  1z9m  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages_df.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how many URLs are matching after pre-processing\n",
    "- the removing duplicates in particular might have caused some URLs to not be in the collection anymore, however, if there is relevant content, it should still be in the passage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_unformatted = list(questions_for_ranking['URLs'])\n",
    "reference_list = []\n",
    "for string in urls_unformatted:\n",
    "    sublist = ast.literal_eval(string)\n",
    "    reference_list.append(sublist)\n",
    "\n",
    "reference_urls = []\n",
    "for sublist in reference_list:\n",
    "    reference_urls.extend(sublist)\n",
    "\n",
    "collected_urls = list(passages_df['URL'])\n",
    "count = 0\n",
    "elements_not_in_collected = []\n",
    "for element in reference_urls:\n",
    "    if element in collected_urls:\n",
    "        count += 1\n",
    "    else:\n",
    "        elements_not_in_collected.append(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count # it was 80 before droping duplicates, so some URLs are not in the collection anymore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_for_ranking['Preprocessed_Question'] = questions_for_ranking['Question'].apply(lambda x: preprocess_text(x,stem=True,\n",
    "                                                                                      remove_stopwords=True,\n",
    "                                                                                      lowercase_text=True,\n",
    "                                                                                      remove_punct=True)) \n",
    "\n",
    "questions_for_ranking['Preprocessed_Answer'] = questions_for_ranking['Answer'].apply(lambda x: preprocess_text(x,stem=True,\n",
    "                                                                                      remove_stopwords=True,\n",
    "                                                                                      lowercase_text=True,\n",
    "                                                                                      remove_punct=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions_for_ranking.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add relevant passage ids to questions \n",
    "- Meaning: add the passages that are part of referenced URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_for_ranking = add_passages_ids(questions_for_ranking, passages_df) # 1:17 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions_for_ranking.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove the questions that have no reference anymore \n",
    "- Idea: so it doesn't bring down retrieval scores + avoid zero division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_for_ranking = questions_for_ranking[questions_for_ranking['passages_ids'].notna()] # remove no reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(questions_for_ranking) # 52 samples left out of around 70"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add question ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashids = Hashids()\n",
    "questions_for_ranking[\"question_id\"] = [hashids.encode(i) for i in range(len(questions_for_ranking))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions_for_ranking.tail() # check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove unused columns - questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions_for_ranking.drop('Unnamed: 0.1', axis=1, inplace=True)\n",
    "#questions_for_ranking.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "#questions_for_ranking.drop('Cleaned_URLs', axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_table = pa.Table.from_pandas(questions_for_ranking)\n",
    "arrow_dict = arrow_table.to_pydict()\n",
    "questions_for_ranking = Dataset.from_dict(arrow_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions_for_ranking[0] # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_table = pa.Table.from_pandas(passages_df)\n",
    "arrow_dict = arrow_table.to_pydict()\n",
    "passages_df = Dataset.from_dict(arrow_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passages_df[1000] # check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(passages_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tfidf = perform_tfidf_search(questions_for_ranking, passages_df, k=100, get_true_passages=True) # 1min:13 without NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_random = perform_random_search(questions_for_ranking, passages_df, k=100, get_true_passages=True) # 18 secs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bm25 = perform_bm25_search(questions_for_ranking, passages_df, k=100, get_true_passages=True) # 1min 24"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Average Answer length\n",
    "- Idea: replicate length with retrieved passages to 'simulate' an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/natalipeeva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/natalipeeva/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/natalipeeva/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "# Set the language to Dutch\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252.8653846153846"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_len = []\n",
    "for q in questions_for_ranking:\n",
    "    ans_len.append(len(word_tokenize(q['Answer'], language='dutch')))\n",
    "sum(ans_len)/len(ans_len) # 256.87 -> make limit for tokens in ranked content 256"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Scores - ranking method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [5, 10, 100]\n",
    "results_lists = [results_tfidf, results_random, results_bm25]\n",
    "retrieval_names = ['TF-IDF', 'Random', 'BM25']\n",
    "\n",
    "table = []\n",
    "\n",
    "for results, retrieval_name in zip(results_lists, retrieval_names):\n",
    "    for k in ks:\n",
    "        row = ['Metrics for k={} (Retrieval: {})'.format(k, retrieval_name)]\n",
    "        average_metrics = calculate_average_metrics_retrieval(results, k)\n",
    "        for metric, value in average_metrics.items():\n",
    "            row.append('{:.4f}'.format(value))\n",
    "        table.append(row)\n",
    "headers = [''] + list(average_metrics.keys())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+----------------------+--------------------+------------------+-------------------------+\n",
      "|                                       |   average_recall@100 |   average_ndcg@100 |   average_rr@100 |   average_precision@100 |\n",
      "+=======================================+======================+====================+==================+=========================+\n",
      "| Metrics for k=5 (Retrieval: TF-IDF)   |               0.0397 |             0.0838 |           0.0663 |                  0.0346 |\n",
      "+---------------------------------------+----------------------+--------------------+------------------+-------------------------+\n",
      "| Metrics for k=10 (Retrieval: TF-IDF)  |               0.0312 |             0.0949 |           0.0781 |                  0.0212 |\n",
      "+---------------------------------------+----------------------+--------------------+------------------+-------------------------+\n",
      "| Metrics for k=100 (Retrieval: TF-IDF) |               0.102  |             0.0961 |           0.0804 |                  0.009  |\n",
      "+---------------------------------------+----------------------+--------------------+------------------+-------------------------+\n",
      "| Metrics for k=5 (Retrieval: Random)   |               0      |             0      |           0      |                  0      |\n",
      "+---------------------------------------+----------------------+--------------------+------------------+-------------------------+\n",
      "| Metrics for k=10 (Retrieval: Random)  |               0      |             0      |           0      |                  0      |\n",
      "+---------------------------------------+----------------------+--------------------+------------------+-------------------------+\n",
      "| Metrics for k=100 (Retrieval: Random) |               0.0019 |             0.003  |           0.0002 |                  0.0002 |\n",
      "+---------------------------------------+----------------------+--------------------+------------------+-------------------------+\n",
      "| Metrics for k=5 (Retrieval: BM25)     |               0.0154 |             0.0493 |           0.0401 |                  0.0154 |\n",
      "+---------------------------------------+----------------------+--------------------+------------------+-------------------------+\n",
      "| Metrics for k=10 (Retrieval: BM25)    |               0.0134 |             0.0568 |           0.0497 |                  0.0115 |\n",
      "+---------------------------------------+----------------------+--------------------+------------------+-------------------------+\n",
      "| Metrics for k=100 (Retrieval: BM25)   |               0.0859 |             0.0763 |           0.0548 |                  0.0067 |\n",
      "+---------------------------------------+----------------------+--------------------+------------------+-------------------------+\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(table, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Scores - Preprocessed ranked text and answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = list(questions_for_ranking['Preprocessed_Answer']) # preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lists = [results_tfidf, results_random, results_bm25]\n",
    "retrieval_names = ['TF-IDF', 'Random', 'BM25']\n",
    "\n",
    "simulated_answers = {}\n",
    "metrics = {}\n",
    "\n",
    "for results, retrieval_name in zip(results_lists, retrieval_names):\n",
    "    simulated_answers[retrieval_name] = []\n",
    "    metrics[retrieval_name] = {}\n",
    "    \n",
    "    for result in results: # generate simulated answers = get top retrieved documents till tokens are < 256\n",
    "        simulated_answer = simulate_answer(result['ranked_text_preprocessed'])\n",
    "        simulated_answers[retrieval_name].append(simulated_answer)\n",
    "    \n",
    "    metric_values = calculate_metrics_answer_similarity(simulated_answers[retrieval_name], answers) # get ROUGE \n",
    "    \n",
    "    metrics[retrieval_name] = metric_values\n",
    "\n",
    "# Print the metrics table\n",
    "table = []\n",
    "headers = ['Retrieval Method'] + list(metric_values.keys())\n",
    "\n",
    "for retrieval_name in retrieval_names:\n",
    "    row = [retrieval_name] + [metrics[retrieval_name][metric] for metric in metric_values.keys()]\n",
    "    table.append(row)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------------+---------------------+--------------+------------+\n",
      "| Retrieval Method   |   ROUGE-1 (Average) |   ROUGE-2 (Average) |   ROUGE-L (Average) |   BLEU Score |   F1 Score |\n",
      "+====================+=====================+=====================+=====================+==============+============+\n",
      "| TF-IDF             |           0.0967257 |          0.0136018  |           0.0571549 |  0.00204189  |          0 |\n",
      "+--------------------+---------------------+---------------------+---------------------+--------------+------------+\n",
      "| Random             |           0.0619725 |          0.00709693 |           0.0355553 |  0.000267472 |          0 |\n",
      "+--------------------+---------------------+---------------------+---------------------+--------------+------------+\n",
      "| BM25               |           0.095807  |          0.0102444  |           0.0535993 |  0.00191856  |          0 |\n",
      "+--------------------+---------------------+---------------------+---------------------+--------------+------------+\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(table, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROUGE Scores - Unpreprocessed ranked text and answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = list(questions_for_ranking['Answer']) # unpreprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lists = [results_tfidf, results_random, results_bm25]\n",
    "retrieval_names = ['TF-IDF', 'Random', 'BM25']\n",
    "\n",
    "simulated_answers = {}\n",
    "metrics = {}\n",
    "\n",
    "for results, retrieval_name in zip(results_lists, retrieval_names):\n",
    "    simulated_answers[retrieval_name] = []\n",
    "    metrics[retrieval_name] = {}\n",
    "    \n",
    "    for result in results: # generate simulated answers = get top retrieved documents till tokens are < 256\n",
    "        simulated_answer = simulate_answer(result['ranked_text']) #unpreprocessed\n",
    "        simulated_answers[retrieval_name].append(simulated_answer)\n",
    "    \n",
    "    metric_values = calculate_metrics_answer_similarity(simulated_answers[retrieval_name], answers) # get ROUGE \n",
    "    \n",
    "    metrics[retrieval_name] = metric_values\n",
    "\n",
    "# Print the metrics table\n",
    "table = []\n",
    "headers = ['Retrieval Method'] + list(metric_values.keys())\n",
    "\n",
    "for retrieval_name in retrieval_names:\n",
    "    row = [retrieval_name] + [metrics[retrieval_name][metric] for metric in metric_values.keys()]\n",
    "    table.append(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------------+---------------------+--------------+------------+\n",
      "| Retrieval Method   |   ROUGE-1 (Average) |   ROUGE-2 (Average) |   ROUGE-L (Average) |   BLEU Score |   F1 Score |\n",
      "+====================+=====================+=====================+=====================+==============+============+\n",
      "| TF-IDF             |            0.216264 |           0.0303644 |            0.112037 |   0.00603454 |          0 |\n",
      "+--------------------+---------------------+---------------------+---------------------+--------------+------------+\n",
      "| Random             |            0.202333 |           0.022343  |            0.107343 |   0.00180064 |          0 |\n",
      "+--------------------+---------------------+---------------------+---------------------+--------------+------------+\n",
      "| BM25               |            0.229851 |           0.0332365 |            0.122055 |   0.00798236 |          0 |\n",
      "+--------------------+---------------------+---------------------+---------------------+--------------+------------+\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(table, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save ranked results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_results(results, question_collection, passages_collection, folder_path):\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    \n",
    "    # Get the variable names of question_collection and passages_collection\n",
    "    question_var_name = [var_name for var_name, var_val in globals().items() if var_val is question_collection][0]\n",
    "    passages_var_name = [var_name for var_name, var_val in globals().items() if var_val is passages_collection][0]\n",
    "\n",
    "    # Save each result variable with its corresponding filename\n",
    "    for result in results:\n",
    "        # Get the variable name of the result variable\n",
    "        result_var_name = [var_name for var_name, var_val in globals().items() if var_val is result][0]\n",
    "        \n",
    "        # Generate the filename based on variable names\n",
    "        filename = '{}_{}_{}.pickle'.format(result_var_name, question_var_name, passages_var_name)\n",
    "        \n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(result, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [results_tfidf, results_bm25, results_random]\n",
    "question_collection = questions_for_ranking\n",
    "passages_collection = passages_df\n",
    "folder_path = 'data/results_ranking'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results, question_collection, passages_collection, folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea4c3bcc219a1292b0d1d9543a9b9f82ed18a35340190a3cbd50b3110bbb4e55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
