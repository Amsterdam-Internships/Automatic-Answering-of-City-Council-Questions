{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install required packages for the notebook:\n",
    "#! pip install gensim==4.3.1\n",
    "#! pip install matplotlib==3.3.2\n",
    "#! pip install pandas==2.0.3\n",
    "#! pip install scikit-learn==1.0.2\n",
    "#! pip install seaborn==0.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the notebook directory\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Get the root directory by navigating upwards two levels\n",
    "root_dir = os.path.dirname(os.path.abspath(os.path.join(notebook_dir, '../../')))\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "os.chdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src_clean.preprocessing.text_preprocessing import preprocess_text\n",
    "from src_clean.eda.visualizations import plot_distribution, plot_most_common, plot_wordcloud_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/question_answer/questions.csv'\n",
    "df_all = pd.read_csv(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df_all['Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = df_all['Answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the questions and answers are pre-processed. The pre-processing step is needed, so that the analysis is performed on data which has less noise (such as stopwords, punctuation etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "        Tokenizes the input text.\n",
    "        Input: text - type(str)\n",
    "        Output: a list of tokens - type(list)\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text, language='dutch')\n",
    "    return tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lengths, Distributions, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_questions = questions.apply(lambda x: preprocess_text(str(x),stem=False,\n",
    "                                                                                      remove_stopwords=True,\n",
    "                                                                                      lowercase_text=True,\n",
    "                                                                                      remove_punct=True)) \n",
    "\n",
    "questions_tokenized = preprocessed_questions.apply(lambda x: tokenize(str(x)))\n",
    "questions_len = questions_tokenized.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(questions_len, 'Question Length', 'Questions', 'Distribution of Question Lengths')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_answers = answers.apply(lambda x: preprocess_text(str(x),stem=False,\n",
    "                                                                                      remove_stopwords=True,\n",
    "                                                                                      lowercase_text=True,\n",
    "                                                                                      remove_punct=True)) \n",
    "\n",
    "answers_tokenized = preprocessed_answers.apply(lambda x: tokenize(str(x)))\n",
    "answers_len = answers_tokenized.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(answers_len, 'Answer Length', 'Answers', 'Distribution of Answer Lengths')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_q = sum(questions_tokenized, [])\n",
    "plot_most_common(corpus_q, 'Question', top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_a = sum(answers_tokenized, [])\n",
    "plot_most_common(corpus_a, 'Answer', top_n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_ngram(corpus, n=None):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx])\n",
    "                  for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_bigrams=get_top_ngram(corpus_q,3)\n",
    "x,y=map(list,zip(*top_n_bigrams)) \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_bigrams=get_top_ngram(corpus_a,2)\n",
    "x,y=map(list,zip(*top_n_bigrams)) \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                             max_df=0.6,\n",
    "                             min_df=3)\n",
    "\n",
    "# Fit-transform the questions\n",
    "tfidf_q = vectorizer.fit_transform(preprocessed_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tf_idf_sum = tfidf_q.sum(axis=0)\n",
    "tf_idf_scores = [(feature_names[i], tf_idf_sum[0, i]) for i in range(len(feature_names))]\n",
    "tf_idf_scores = sorted(tf_idf_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 10 most important words in the corpus\n",
    "for term, score in tf_idf_scores[:20]:\n",
    "    print(f\"{term}: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,3),\n",
    "                             max_df=0.6,\n",
    "                             min_df=3)\n",
    "tfidf_a = vectorizer.fit_transform(preprocessed_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tf_idf_sum = tfidf_a.sum(axis=0)\n",
    "tf_idf_scores = [(feature_names[i], tf_idf_sum[0, i]) for i in range(len(feature_names))]\n",
    "tf_idf_scores = sorted(tf_idf_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the top 10 most important words in the corpus\n",
    "for term, score in tf_idf_scores[:20]:\n",
    "    print(f\"{term}: {score:.2f}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove some additional frequent words to make the topics more clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = ['welke',\n",
    " 'amsterdam',\n",
    " 'waarom',\n",
    " 'gemeente',\n",
    " 'nee',\n",
    " 'hoeveel',\n",
    " 'bereid',\n",
    " 'college',\n",
    " 'gaat',\n",
    " 'bekend',\n",
    " 'graag',\n",
    " 'fractie',\n",
    " 'aangeven',\n",
    " 'mening',\n",
    " 'amsterdamse',\n",
    " 'toelichting',\n",
    " 'klopt',\n",
    " 'gaan',\n",
    " 'mogelijk',\n",
    " 'footnotestart',\n",
    " 'footnoteend']\n",
    "\n",
    "def remove_stopwords(words, stopwords):\n",
    "    return [word for word in words if word not in stopwords]\n",
    "\n",
    "questions_tokenized = questions_tokenized.apply(lambda x: remove_stopwords(x, words_to_remove))\n",
    "\n",
    "def remove_short_words(word_list):\n",
    "    return list(filter(lambda word: len(word) > 3, word_list))\n",
    "\n",
    "questions_tokenized = questions_tokenized.apply(lambda word_list: remove_short_words(word_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import math\n",
    "from matplotlib import colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud_grid(lda_model, num_topics, num_words, ncols, width=4, height=3):\n",
    "    \"\"\"\n",
    "    Create a grid of word clouds for multiple topics from an LDA model.\n",
    "    \n",
    "    Parameters:\n",
    "    - lda_model: Trained LDA model.\n",
    "    - num_topics: Number of topics to display.\n",
    "    - num_words: Number of top words to include in each topic's word cloud.\n",
    "    - ncols: Number of columns in the grid.\n",
    "    - width: Width of each word cloud plot (default: 4).\n",
    "    - height: Height of each word cloud plot (default: 4).\n",
    "    \"\"\"\n",
    "    nb_rows = math.ceil(num_topics / ncols)\n",
    "    \n",
    "    cols = [color for name, color in mcolors.TABLEAU_COLORS.items()] \n",
    "    cols = cols * math.ceil(num_topics / len(cols))\n",
    "\n",
    "    cloud = WordCloud(background_color='white',\n",
    "                      width=400,\n",
    "                      height=400,\n",
    "                      max_words=num_words,\n",
    "                      color_func=lambda *args, **kwargs: cols[i],\n",
    "                      prefer_horizontal=1.0)\n",
    "\n",
    "    topics = lda_model.show_topics(num_topics=num_topics, num_words=num_words, formatted=False)\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=ncols, nrows=nb_rows, \n",
    "                             figsize=(width*ncols, height*nb_rows), \n",
    "                             sharex=True, sharey=True)\n",
    "\n",
    "    for i, (topic, ax) in enumerate(zip(topics, axes.flatten())):\n",
    "        topic_words = dict(topic[1])\n",
    "        cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "        ax.imshow(cloud)\n",
    "        #ax.set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.axis('off')\n",
    "    plt.margins(x=0, y=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic=gensim.corpora.Dictionary(questions_tokenized)\n",
    "bow_corpus = [dic.doc2bow(doc) for doc in questions_tokenized]\n",
    "\n",
    "lda_model_q = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                   num_topics = 18,\n",
    "                                   id2word = dic,\n",
    "                                   passes = 10,\n",
    "                                   workers = 2, \n",
    "                                        random_state=30)\n",
    "\n",
    "num_topics = 18\n",
    "num_words = 10\n",
    "ncols = 6\n",
    "\n",
    "plot_wordcloud_grid(lda_model_q, num_topics, num_words, ncols)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic=gensim.corpora.Dictionary(answers_tokenized)\n",
    "bow_corpus = [dic.doc2bow(doc) for doc in answers_tokenized]\n",
    "\n",
    "lda_model_a = gensim.models.LdaMulticore(bow_corpus,\n",
    "                                   num_topics = 20,\n",
    "                                   id2word = dic,\n",
    "                                   passes = 10,\n",
    "                                   workers = 2, \n",
    "                                        random_state=30)\n",
    "\n",
    "num_topics = 18\n",
    "num_words = 10\n",
    "ncols = 6\n",
    "\n",
    "plot_wordcloud_grid(lda_model_a, num_topics, num_words, ncols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating Outliers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['Question Len'] = questions_len\n",
    "df_all['Answer Len'] = answers_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean is very different from the max values, which indicates outliers, regardless of the fact that mean is highly affected by extreme values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_IQR(df):\n",
    "    q1=df.quantile(0.25)\n",
    "    \n",
    "    q3=df.quantile(0.75)\n",
    "    \n",
    "    IQR=q3-q1\n",
    "    \n",
    "    outliers = df[((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))]\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_q = find_outliers_IQR(df_all['Question Len']).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_a = find_outliers_IQR(df_all['Answer Len']).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(outliers_q))\n",
    "print(len(outliers_a))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do questions start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_questions_no_stem = questions.apply(lambda x: preprocess_text(str(x),stem=False,\n",
    "                                                                                      remove_stopwords=False,\n",
    "                                                                                      lowercase_text=True,\n",
    "                                                                                      remove_punct=True)) \n",
    "questions_tokenized_no_stem = preprocessed_questions_no_stem.apply(lambda x: tokenize(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_start = [q[0] for q in questions_tokenized_no_stem]\n",
    "counted = Counter(q_start)\n",
    "x, y = zip(*counted.most_common(25))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=list(y), y=list(x), color='red')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Question Start')\n",
    "plt.title('Most Common Question Starts')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Opinion and Factual Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_words = {\n",
    "    'mening': 0, 'convictie': 0, 'denkbeeld': 0, 'denkwijs': 0, 'denkwijze': 0, 'dunk': 0,\n",
    "    'gedacht': 0, 'gedachte': 0, 'geest': 0, 'gevoelen': 0, 'gezindheid': 0, 'idee': 0,\n",
    "    'inzicht': 0, 'inzien': 0, 'kijk': 0, 'oordeel': 0, 'opinie': 0, 'bevindingen': 0,\n",
    "    'besluiten': 0, 'beslissend': 0, 'stellingname': 0, 'visie': 0, 'zienswijze': 0,\n",
    "    'zin': 0, 'bekend': 0, 'college': 0, 'vindt': 0,\n",
    "    'standpunt': 0, 'bereid': 0, 'kennisgenomen': 0\n",
    "}\n",
    "\n",
    "opinion_counts = defaultdict(int)\n",
    "\n",
    "for q in questions_tokenized_no_stem:\n",
    "    found_opinions = [w for w in opinion_words if w in q]\n",
    "    for opinion in found_opinions:\n",
    "        opinion_counts[opinion] += 1\n",
    "\n",
    "opinion_counts = dict(opinion_counts)\n",
    "print(opinion_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "import types\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "            \n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system/pip names. Unfortunately,\n",
    "        # there is no systematic way to get pip names from\n",
    "        # a package's imported name. You'll have to add\n",
    "        # exceptions to this list manually!\n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "            \n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "# The only way I found to get the version of the root package\n",
    "# from only the name of the package is to cross-check the names \n",
    "# of installed packages vs. imported packages\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea4c3bcc219a1292b0d1d9543a9b9f82ed18a35340190a3cbd50b3110bbb4e55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
