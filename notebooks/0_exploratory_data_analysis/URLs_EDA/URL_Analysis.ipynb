{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is going to do exploratory analysis on the URLs which have been used as a reference in some of the answers from the MunicipalQA dataset.\n",
    "\n",
    "The following analysis has been performed:\n",
    "\n",
    "1. Most frequent URL domains\n",
    "2. Most frequent main paths of the corresponding domain\n",
    "3. Topics in the referenced URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the notebook directory\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Get the root directory by navigating upwards two levels\n",
    "root_dir = os.path.dirname(os.path.abspath(os.path.join(notebook_dir, '../../')))\n",
    "\n",
    "# Change the current working directory to the root directory\n",
    "os.chdir(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from yarl import URL\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/question_answer/questions.csv'\n",
    "questions = pd.read_csv(open(data_dir, 'r'))\n",
    "urls = sum(map(lambda x: x.split('\\n'), questions[questions['URLs'].notnull()]['URLs']), [])\n",
    "urls = list(map(lambda x: x if x.startswith('http') else f'https://{x}', urls))\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions['Year'] = pd.to_numeric(questions['Year'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions['Year'].dropna().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions['Year'].dropna().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(questions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make URLs from URL type - yarl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.DataFrame()\n",
    "sample['url'] = urls\n",
    "sample['url'] = sample['url'].apply(lambda url: URL(url))\n",
    "sample['path'] =sample.url.apply(lambda url: url.path)\n",
    "sample['host'] =sample.url.apply(lambda url: url.host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The most common domains are:')\n",
    "print()\n",
    "print(sample['host'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the top 20 most common domains and sort them\n",
    "top_domains = sample['host'].value_counts().head(5).sort_values(ascending=True)\n",
    "\n",
    "# Create a horizontal bar plot\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "plt.barh(top_domains.index, top_domains.values)  # Create the horizontal bar plot\n",
    "plt.xlabel('Frequency', fontsize=20)  # Set the x-axis label with fontsize\n",
    "plt.ylabel('Domains', fontsize=20)  # Set the y-axis label with fontsize\n",
    "plt.title('Most Common Domains', fontsize=20)  # Set the title with fontsize\n",
    "plt.xticks(fontsize=20)  # Set the font size of the x-axis ticks\n",
    "plt.yticks(fontsize=20)  # Set the font size of the y-axis ticks\n",
    "\n",
    "# Save the plot in high quality\n",
    "plt.savefig('bar_plot.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check URL (file) extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_count = 0\n",
    "non_html_count = 0\n",
    "\n",
    "extensions = []\n",
    "\n",
    "for url in urls:\n",
    "    # Extract the file extension from the URL\n",
    "    file_ext = os.path.splitext(url)[1]\n",
    "    extensions.append(file_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(extensions).most_common(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some .pdf which should be accounted for during collection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect amsterdam.nl paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_string(url, domain):\n",
    "    \"\"\"\n",
    "    Extracts the string between 'amsterdam.nl/' and the next '/' in a URL.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_index = url.find(domain + '/') + len(domain + '/')\n",
    "    end_index = url.find('/', start_index)\n",
    "    if end_index == -1:\n",
    "        end_index = len(url)\n",
    "    return url[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Amsterdam.nl ###########\n",
    "paths = []\n",
    "for url in urls:\n",
    "    if 'www.amsterdam.nl' in url:\n",
    "        paths.append(extract_string(url, 'amsterdam.nl'))\n",
    "\n",
    "Counter(paths).most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Rijksoverheid ###########\n",
    "paths = []\n",
    "for url in urls:\n",
    "    if 'www.rijksoverheid.nl' in url:\n",
    "        paths.append(extract_string(url, 'rijksoverheid.nl'))\n",
    "\n",
    "Counter(paths).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### amsterdam.raadsinformatie.nl ###########\n",
    "paths = []\n",
    "for url in urls:\n",
    "    if 'amsterdam.raadsinformatie.nl' in url:\n",
    "        paths.append(extract_string(url, 'amsterdam.raadsinformatie.nl'))\n",
    "\n",
    "Counter(paths).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### www.parool.nl  ###########\n",
    "paths = []\n",
    "for url in urls:\n",
    "    if 'www.parool.nl' in url:\n",
    "        paths.append(extract_string(url, 'parool.nl'))\n",
    "\n",
    "Counter(paths).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### www.rivm.nl  ###########\n",
    "paths = []\n",
    "for url in urls:\n",
    "    if 'www.rivm.nl' in url:\n",
    "        paths.append(extract_string(url, 'rivm.nl'))\n",
    "\n",
    "Counter(paths).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### www.ggd.amsterdam.nl  ###########\n",
    "paths = []\n",
    "for url in urls:\n",
    "    if 'www.ggd.amsterdam.nl' in url:\n",
    "        paths.append(extract_string(url, 'ggd.amsterdam.nl'))\n",
    "\n",
    "Counter(paths).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### www.tweedekamer.nl  ###########\n",
    "paths = []\n",
    "for url in urls:\n",
    "    if 'www.tweedekamer.nl' in url:\n",
    "        paths.append(extract_string(url, 'tweedekamer.nl'))\n",
    "\n",
    "Counter(paths).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### www.infomil.nl  ###########\n",
    "paths = []\n",
    "for url in urls:\n",
    "    if 'www.infomil.nl' in url:\n",
    "        paths.append(extract_string(url, 'infomil.nl'))\n",
    "\n",
    "Counter(paths).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### www.infomil.nl  ###########\n",
    "paths = []\n",
    "for url in urls:\n",
    "    if 'data.amsterdam.nl' in url:\n",
    "        paths.append(extract_string(url, 'data.amsterdam.nl'))\n",
    "\n",
    "Counter(paths).most_common()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be sure of the factual validity of our corpus we are going to collect supporting documents only from "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "1. The most frequently used domains as a reference are: amsterdam.nl, rijksoverheid.nl, rivm.nl, etc. \n",
    "2. The most frequent sub-domains were analyzed and would be taken into account during collection. \n",
    "\n",
    "\n",
    "\n",
    "**Additional findings after manual exploration**:\n",
    "\n",
    "1. Some URL paths are different at their current version than the version that has been used at the time of referencing \n",
    "2. Some URLs appear to be directing to a non-existent page\n",
    "3. A common error that results in a URL being broken is a wrong ending, which is either a \".\" or a \").\"\n",
    "\n",
    "# Actions to take\n",
    "1. Build a collection of supporting documents based on the most common domains and URL paths (url, html_content)\n",
    "2. Update the URLs of the references with their most current versions \n",
    "3. Clean the URLs if needed (e.g. if they end with a '.')\n",
    "4. Collect the HTML content of the refrence URLs as well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea4c3bcc219a1292b0d1d9543a9b9f82ed18a35340190a3cbd50b3110bbb4e55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
