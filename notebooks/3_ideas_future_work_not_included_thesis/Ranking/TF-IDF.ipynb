{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/natalipeeva/Documents/GitHub/Automatic-Answering-of-City-Council-Questions\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/natalipeeva/Documents/GitHub/Automatic-Answering-of-City-Council-Questions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "src_dir = os.path.join(os.getcwd(), 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from read_data.read_data import read_urls_questions, get_questions, get_url_content_tuples, get_relevant_docs\n",
    "from retrieval.sparse_retrieval.tfidf import perform_tfidf_search\n",
    "from retrieval.evaluation.evaluate import calculate_recall_at_k, calculate_average_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected, questions =  read_urls_questions('data/reference_urls/reference_urls_collected.csv',\n",
    "                                            'data/question_answer/questions_updated_urls.csv',\n",
    "                                            clean_url_nan=True) # read collected urls and questions + remove unsuccessful collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = get_questions(questions)\n",
    "document_list = get_url_content_tuples(collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = perform_tfidf_search(question_list, document_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = get_relevant_docs(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_k(results, relevant_docs, k):\n",
    "    \"\"\"\n",
    "    Compute Recall@K\n",
    "    Input:\n",
    "        results: A sorted list of tuples (document_id, score), with the most relevant document in the first position\n",
    "        relevant_docs: A set of relevant documents.\n",
    "        k: the cut-off\n",
    "    Output: Recall@K\n",
    "    \"\"\"\n",
    "    # Get unique document IDs from the results and relevant_docs\n",
    "    unique_results = list(set(result[0] for result in results))\n",
    "    unique_relevant_docs = list(set(relevant_docs))\n",
    "\n",
    "    if k > len(unique_results):\n",
    "        k = len(unique_results)\n",
    "\n",
    "    relevant_count = 0\n",
    "    for i in range(k):\n",
    "        if unique_results[i] in unique_relevant_docs:  # Check if result is in relevant\n",
    "            relevant_count += 1\n",
    "\n",
    "    k_recall = float(relevant_count) / float(len(unique_relevant_docs))\n",
    "\n",
    "    return k_recall\n",
    "\n",
    "\n",
    "def get_k_urls(bm25_results):\n",
    "    urls = []\n",
    "    counter = 1\n",
    "    for result in bm25_results:\n",
    "\n",
    "        urls.append((result['url'], result['score']))\n",
    "        counter+=1\n",
    "    return urls\n",
    "\n",
    "def calculate_recall_at_k(queries, search_results, relevant_docs, k_values, ranking_method):\n",
    "    \"\"\"\n",
    "    Calculate Recall@K for a list of queries and specified values of k.\n",
    "    Input:\n",
    "        queries - a list of queries\n",
    "        collection: a list of tuples (document_id, document_content)\n",
    "        relevant_docs: a dictionary where the key is the query and the value is a set of relevant document IDs\n",
    "        k_values: a list of k values for recall calculation\n",
    "    Output: a dictionary where the key is the query and the value is a dictionary of recall values at each k\n",
    "    \"\"\"\n",
    "    recall_results = {}\n",
    "\n",
    "    for query in queries:\n",
    "        if ranking_method=='bm25':\n",
    "            results = get_k_urls(search_results[query][1])\n",
    "        elif ranking_method=='tf-idf':\n",
    "            results = search_results[query]\n",
    "        relevant = relevant_docs[query]\n",
    "\n",
    "        recall_values = {}\n",
    "        for k in k_values:\n",
    "            recall = recall_k(results, relevant, k)\n",
    "            recall_values[k] = recall\n",
    "\n",
    "        recall_results[query] = recall_values\n",
    "\n",
    "    return recall_results\n",
    "\n",
    "\n",
    "def calculate_average_recall(recall_results):\n",
    "    \"\"\"\n",
    "    Calculate the average recall values per recall type from a dictionary of recall values.\n",
    "    Input:\n",
    "        recall_results: a dictionary where the key is the query and the value is a dictionary of recall values at each k\n",
    "    Output: a dictionary where the key is the recall type and the value is the average recall value\n",
    "    \"\"\"\n",
    "    average_recall = {}\n",
    "\n",
    "    for query_recall in recall_results.values():\n",
    "        for k, recall in query_recall.items():\n",
    "            if k not in average_recall:\n",
    "                average_recall[k] = 0.0\n",
    "            average_recall[k] += recall\n",
    "\n",
    "    query_count = len(recall_results)\n",
    "    for k in average_recall:\n",
    "        average_recall[k] /= query_count\n",
    "\n",
    "    return average_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = calculate_recall_at_k(question_list, all_results, relevant_docs, [1, 5, 10], 'tf-idf') # think of a metric similarity recall? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.007352941176470588, 5: 0.0428921568627451, 10: 0.07598039215686274}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_average_recall(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Reciprocal Rank: 0.009435190005205622\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for question in all_results.keys():\n",
    "    urls = []\n",
    "    for result in all_results[question]:\n",
    "        urls.append(result)\n",
    "    predictions.append(list(set(urls)))\n",
    "\n",
    "\n",
    "true = []\n",
    "for question in relevant_docs.keys():\n",
    "    true.append(list(set(relevant_docs[question])))\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from irmetrics.topk import rr\n",
    "\n",
    "# Calculate the Mean Reciprocal Rank for each question\n",
    "mrr_values = []\n",
    "for i in range(len(predictions)):\n",
    "    true_values = true[i]\n",
    "    mrr = rr(true_values, predictions[i])\n",
    "    mrr_values.append(mrr)\n",
    "\n",
    "# Calculate the average Mean Reciprocal Rank\n",
    "average_mrr = np.mean(mrr_values)\n",
    "\n",
    "print(\"Average Mean Reciprocal Rank:\", average_mrr)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Reciprocal Rank: 0.008849557522123894\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from irmetrics.topk import recall\n",
    "\n",
    "# Calculate the Mean Reciprocal Rank for each question\n",
    "mrr_values = []\n",
    "for i in range(len(predictions)):\n",
    "    true_values = true[i]\n",
    "    mrr = recall(true_values, predictions[i], k=10)\n",
    "    mrr_values.append(mrr)\n",
    "\n",
    "# Calculate the average Mean Reciprocal Rank\n",
    "average_recall = np.mean(mrr_values)\n",
    "\n",
    "print(\"Average Mean Reciprocal Rank:\", average_recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run to save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # if save\n",
    "with open ('data/results/tfidf_results.pickle', 'wb') as f:\n",
    "    pickle.dump(all_results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea4c3bcc219a1292b0d1d9543a9b9f82ed18a35340190a3cbd50b3110bbb4e55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
