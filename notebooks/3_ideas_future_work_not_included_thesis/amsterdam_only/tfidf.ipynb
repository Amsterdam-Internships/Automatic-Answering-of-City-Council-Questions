{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/natalipeeva/Documents/GitHub/Automatic-Answering-of-City-Council-Questions\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/natalipeeva/Documents/GitHub/Automatic-Answering-of-City-Council-Questions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "src_dir = os.path.join(os.getcwd(), 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from read_data.read_data import read_urls_questions, get_questions, get_url_content_tuples, get_relevant_docs\n",
    "from retrieval.sparse_retrieval.tfidf import perform_tfidf_search\n",
    "from retrieval.evaluation.evaluate import calculate_recall_at_k, calculate_average_recall\n",
    "import pandas as pd\n",
    "from irmetrics.topk import recall, rr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected, questions =  read_urls_questions(os.path.join('data/reference_urls/reference_urls_collected.csv'),\n",
    "                                            os.path.join('data/question_answer/questions_updated_urls.csv'), clean_url_nan=True) # read collected urls and questions + remove unsuccessful collection\n",
    "\n",
    "amsterdam_only = questions[questions['Cleaned_URLs'].apply(lambda urls: any(url.startswith('https://www.amsterdam.nl') for url in urls))]\n",
    "amsterdam_collected = pd.read_csv('data/collected/combined.csv')\n",
    "amsterdam_references = collected[collected['URL'].str.startswith('https://www.amsterdam.nl')]\n",
    "amsterdam_missing = pd.DataFrame()  # Empty DataFrame to store missing rows\n",
    "missing_rows = []\n",
    "\n",
    "for index, row in amsterdam_references.iterrows():\n",
    "    if row['URL'] not in amsterdam_collected['URL'].tolist():\n",
    "        missing_rows.append(row.to_dict())\n",
    "\n",
    "amsterdam_missing = pd.DataFrame(missing_rows)\n",
    "all_amsterdam = pd.concat([amsterdam_collected, amsterdam_missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = get_questions(amsterdam_only)\n",
    "document_list = get_url_content_tuples(all_amsterdam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = perform_tfidf_search(question_list, document_list, k=100) # took 5 minutes!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n7.\\nKan het college de reeds bestaande zwemplekken in Amsterdam en de directe \\nomgeving, die zich op fietsafstand bevinden, beter communiceren zodat mensen \\nweten waar ze allemaal heen kunnen op de fiets om te zwemmen?\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.amsterdam.nl/veelgevraagd/?productid=%7BC9BE6316-1680-402C-A15E-91FA8D66BD82%7D'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results[list(all_results.keys())[0]][3][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Reciprocal Rank: 0.00014269406392694063\n",
      "Average Recall: 0.0136986301369863\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = get_relevant_docs(questions)\n",
    "\n",
    "predictions = []\n",
    "for question in all_results.keys():\n",
    "    urls = []\n",
    "    for result in all_results[question]:\n",
    "        urls.append(result[1])\n",
    "    predictions.append(list(set(urls)))\n",
    "\n",
    "\n",
    "true = []\n",
    "for question in relevant_docs.keys():\n",
    "    true.append(list(set(relevant_docs[question])))\n",
    "\n",
    "\n",
    "# Calculate the Mean Reciprocal Rank for each question\n",
    "mrr_values = []\n",
    "for i in range(len(predictions)):\n",
    "    true_values = true[i]\n",
    "    mrr = rr(true_values, predictions[i], k=100)\n",
    "    mrr_values.append(mrr)\n",
    "\n",
    "# Calculate the average Mean Reciprocal Rank\n",
    "average_mrr = np.mean(mrr_values)\n",
    "\n",
    "print(\"Average Mean Reciprocal Rank:\", average_mrr)\n",
    "\n",
    "\n",
    " # Calculate the Mean Reciprocal Rank for each question\n",
    "mrr_values = []\n",
    "for i in range(len(predictions)):\n",
    "    true_values = true[i]\n",
    "    mrr = recall(true_values, predictions[i], k=100)\n",
    "    mrr_values.append(mrr)\n",
    "\n",
    "# Calculate the average Mean Reciprocal Rank\n",
    "average_recall = np.mean(mrr_values)\n",
    "\n",
    "print(\"Average Recall:\", average_recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/natalipeeva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/natalipeeva/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/natalipeeva/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources for Dutch tokenization\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe for the new data\n",
    "new_df = pd.DataFrame(columns=['URL', 'Textual_Content'])\n",
    "\n",
    "# Iterate over each row in the original dataframe\n",
    "for index, row in all_amsterdam.iterrows():\n",
    "    url = row['URL']\n",
    "    content = str(row['Textual_Content'])\n",
    "    \n",
    "    # Tokenize the content into words\n",
    "    words = word_tokenize(content, language='dutch')\n",
    "    \n",
    "    # Create passages of 100 words\n",
    "    passages = [' '.join(words[i:i+100]) for i in range(0, len(words), 100)]\n",
    "    \n",
    "    # Create a new dataframe for the passages\n",
    "    passage_df = pd.DataFrame({'URL': url, 'Textual_Content': passages})\n",
    "    \n",
    "    # Concatenate the new dataframe with the main dataframe\n",
    "    new_df = pd.concat([new_df, passage_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = get_questions(amsterdam_only)\n",
    "document_list = get_url_content_tuples(new_df)\n",
    "all_results = perform_tfidf_search(question_list, document_list, k=100) # took 5 minutes!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Mean Reciprocal Rank: 0.0\n",
      "Average Recall: 0.0\n"
     ]
    }
   ],
   "source": [
    "relevant_docs = get_relevant_docs(questions)\n",
    "\n",
    "predictions = []\n",
    "for question in all_results.keys():\n",
    "    urls = []\n",
    "    for result in all_results[question]:\n",
    "        urls.append(result[1])\n",
    "    predictions.append(list(set(urls)))\n",
    "\n",
    "\n",
    "true = []\n",
    "for question in relevant_docs.keys():\n",
    "    true.append(list(set(relevant_docs[question])))\n",
    "\n",
    "\n",
    "# Calculate the Mean Reciprocal Rank for each question\n",
    "mrr_values = []\n",
    "for i in range(len(predictions)):\n",
    "    true_values = true[i]\n",
    "    mrr = rr(true_values, predictions[i], k=100)\n",
    "    mrr_values.append(mrr)\n",
    "\n",
    "# Calculate the average Mean Reciprocal Rank\n",
    "average_mrr = np.mean(mrr_values)\n",
    "\n",
    "print(\"Average Mean Reciprocal Rank:\", average_mrr)\n",
    "\n",
    "\n",
    " # Calculate the Mean Reciprocal Rank for each question\n",
    "mrr_values = []\n",
    "for i in range(len(predictions)):\n",
    "    true_values = true[i]\n",
    "    mrr = recall(true_values, predictions[i], k=100)\n",
    "    mrr_values.append(mrr)\n",
    "\n",
    "# Calculate the average Mean Reciprocal Rank\n",
    "average_recall = np.mean(mrr_values)\n",
    "\n",
    "print(\"Average Recall:\", average_recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcomes: \n",
    "\n",
    "Passages give 0 accuracy \n",
    "TF-IDF very bad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea4c3bcc219a1292b0d1d9543a9b9f82ed18a35340190a3cbd50b3110bbb4e55"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
